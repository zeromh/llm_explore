{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_model_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"knkarthick/dialogsum\"\n",
    "MODEL_NAME = \"google/flan-t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 247577856\n",
      "Trainable parameters: 247577856\n",
      "Percentage of trainable parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "total_params, trainable_params = print_number_of_model_parameters(model)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Percentage of trainable parameters: {trainable_params / total_params * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_n_shot_summary_prompt(example_ids=None, summarize_id=0, data=dataset, my_set='test'):\n",
    "    prompt = ''\n",
    "    if example_ids:\n",
    "        for i in example_ids:\n",
    "            dialogue = data[my_set]['dialogue'][i]\n",
    "            human_summary = data[my_set]['summary'][i]\n",
    "    \n",
    "            prompt += f\"\"\"\n",
    "DIALOGUE:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "SUMMARY:\n",
    "\n",
    "{human_summary}\n",
    "\"\"\"\n",
    "        \n",
    "    dialogue = data[my_set]['dialogue'][summarize_id]\n",
    "\n",
    "    prompt += f\"\"\"\n",
    "DIALOGUE:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "    \n",
    "\n",
    "def get_model_completion(prompt, tokenizer=tokenizer, model=model, gen_config=None):\n",
    "    sentence_encoded = tokenizer(prompt, return_tensors='pt')\n",
    "    completion = model.generate(sentence_encoded.input_ids,\n",
    "                               num_beams=1,\n",
    "                               do_sample=True,\n",
    "                               max_new_tokens=1000,\n",
    "                               generation_config=gen_config)[0]\n",
    "    return tokenizer.decode(completion, skip_special_tokens=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIALOGUE:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "SUMMARY:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = make_n_shot_summary_prompt(summarize_id=200, data=dataset)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Person2#: What do you want to do as someone else mentioned?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'DIA', 'LOG', 'UE', ':', '▁#', 'P', 'erson', '1', '#', ':', '▁Have', '▁you', '▁considered', '▁upgrading', '▁your', '▁system', '?', '▁#', 'P', 'erson', '2', '#', ':', '▁Yes', ',', '▁but', '▁I', \"'\", 'm', '▁not', '▁sure', '▁what', '▁exactly', '▁I', '▁would', '▁need', '.', '▁#', 'P', 'erson', '1', '#', ':', '▁You', '▁could', '▁consider', '▁adding', '▁', 'a', '▁painting', '▁program', '▁to', '▁your', '▁software', '.', '▁It', '▁would', '▁allow', '▁you', '▁to', '▁make', '▁up', '▁your', '▁own', '▁fly', 'ers', '▁and', '▁banner', 's', '▁for', '▁advertising', '.', '▁#', 'P', 'erson', '2', '#', ':', '▁That', '▁would', '▁be', '▁', 'a', '▁', 'definite', '▁bonus', '.', '▁#', 'P', 'erson', '1', '#', ':', '▁You', '▁might', '▁also', '▁want', '▁to', '▁upgrade', '▁your', '▁hardware', '▁because', '▁it', '▁is', '▁pretty', '▁outdated', '▁now', '.', '▁#', 'P', 'erson', '2', '#', ':', '▁How', '▁can', '▁we', '▁do', '▁that', '?', '▁#', 'P', 'erson', '1', '#', ':', '▁You', \"'\", 'd', '▁probably', '▁need', '▁', 'a', '▁faster', '▁processor', ',', '▁to', '▁begin', '▁with', '.', '▁And', '▁you', '▁also', '▁need', '▁', 'a', '▁more', '▁powerful', '▁hard', '▁disc', ',', '▁more', '▁memory', '▁and', '▁', 'a', '▁faster', '▁mode', 'm', '.', '▁Do', '▁you', '▁have', '▁', 'a', '▁CD', '-', 'ROM', '▁drive', '?', '▁#', 'P', 'erson', '2', '#', ':', '▁No', '.', '▁#', 'P', 'erson', '1', '#', ':', '▁The', 'n', '▁you', '▁might', '▁want', '▁to', '▁add', '▁', 'a', '▁CD', '-', 'ROM', '▁drive', '▁too', ',', '▁because', '▁most', '▁new', '▁software', '▁programs', '▁are', '▁coming', '▁out', '▁on', '▁C', 'd', 's', '.', '▁#', 'P', 'erson', '2', '#', ':', '▁That', '▁sounds', '▁great', '.', '▁Thanks', '.', '▁', 'SU', 'MM', 'ARY', ':', '▁', '</s>']\n"
     ]
    }
   ],
   "source": [
    "sentence_encoded = tokenizer(prompt, return_tensors='pt')\n",
    "tokens = tokenizer.convert_ids_to_tokens(sentence_encoded.input_ids[0])\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = \"Summarize the following conversation.\\n\\n\"\n",
    "    end_prompt = \"\\n\\nSummary: \"\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example['dialogue']]\n",
    "    example['input_ids'] = tokenizer(prompt, truncation=True, padding='max_length', return_tensors='pt').input_ids\n",
    "    example['labels'] = tokenizer(example['summary'], truncation=True, padding='max_length', return_tensors='pt').input_ids\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1e99e2cc094a1d8604f780e615d952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['id', 'topic', 'dialogue', 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3c787f83624094ad23092972a22651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1d81c5968e4bb9863b19c72cfe52fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c649912362f846179cdc124525a03399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset_small = tokenized_dataset.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 125\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 15\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_explore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
