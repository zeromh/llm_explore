{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2026783f",
   "metadata": {},
   "source": [
    "# Evaluating models\n",
    "\n",
    "### Things the could should do\n",
    "1. Load model\n",
    "1. Qualitative eval (prompt it)\n",
    "1. Eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6ee2389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import importlib\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "\n",
    "from llm_explore import utils\n",
    "from llm_explore.definitions import ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "792bb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User sets dataset and model names\n",
    "DATASET_NAME = \"knkarthick/dialogsum\"\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "PEFT_MODEL_FILE = Path(ROOT_DIR, \"models\", \"peft-dialogue-summary-training-2025-05-08_21-14-28/checkpoint-2000/\")\n",
    "ADAPTER_NAME = \"chk-2000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f245b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned MPS device\n"
     ]
    }
   ],
   "source": [
    "device = utils.get_torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a437cefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and Model Initialization\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16).to(device)\n",
    " # Keep a copy of the original model for later use (keep on CPU for now)\n",
    "model_orig = deepcopy(model).to(torch.device(\"cpu\"))\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model,\n",
    "    PEFT_MODEL_FILE,\n",
    "    torch_device=device,\n",
    "    adapter_name=ADAPTER_NAME,\n",
    "    is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80ae2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load another adapter\n",
    "path = Path(ROOT_DIR, \"models\", \"peft-dialogue-summary-training-2025-05-05_08-03-52/checkpoint-1246/\")\n",
    "peft_model.load_adapter(path, adapter_name=\"chk-1246\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a3ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenization and Dataset Preparation\n",
    "def tokenize_function(example):\n",
    "    \"\"\"Tokenizes the input and output text for the model, \n",
    "    including a hardcoded prompt to summarize the conversation.\"\"\"\n",
    "    \n",
    "    start_prompt = \"Summarize the following conversation.\\n\\n\"\n",
    "    end_prompt = \"\\n\\nSummary: \"\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example['dialogue']]\n",
    "    output = tokenizer(prompt, truncation=True, padding='max_length', return_tensors='pt')\n",
    "    output['labels'] = tokenizer(example['summary'], truncation=True, padding='max_length', return_tensors='pt').input_ids\n",
    "    return output\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['id', 'topic', 'dialogue', 'summary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e0caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Well, I'll see you later, Mrs. Todd. My wife is waiting for me to take her shopping.\n",
      "#Person2#: I understand. There's a lot to get done at weekends, especially when you two work and the children are small.\n",
      "#Person1#: That's right. Jane and I have been talking about visiting you. So when I saw you in the garden, I decided to come over and say hello.\n",
      "#Person2#: I'm glad you did. In fact, I should have called on you first, since you have newly moved here.\n",
      "#Person1#: By the way, do you need anything from the store?\n",
      "#Person2#: No, but thanks for the offer. And thank you for coming over.\n",
      "#Person1#: It's a pleasure.\n",
      "\n",
      "Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_id = 102\n",
    "prompt = utils.make_n_shot_summary_prompt(summarize_id=my_id, data=dataset)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "846b481c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jane wants to visit her son but Jane's wife's waiting to take her shopping. Mr. Todd encourages her. Mrs. Todd offers her a free gift.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mps.manual_seed(42)\n",
    "sentence_encoded = tokenizer(prompt, return_tensors='pt').to('mps')  # Move the entire batch to MPS\n",
    "model_orig = model_orig.to(device) \n",
    "peft_model.set_adapter(\"chk-2000\")\n",
    "completion = peft_model.generate(input_ids=sentence_encoded.input_ids,\n",
    "                            num_beams=1,\n",
    "                            do_sample=True,\n",
    "                            max_new_tokens=1000,\n",
    "                            generation_config=None)[0]  # No need to call .to('mps') again\n",
    "tokenizer.decode(completion, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8097d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mrs. Todd will see #Person1# at the weekend while the wife is busy shopping. Mrs. Todd decides to visit her in the garden.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.set_adapter(\"chk-1246\")\n",
    "completion = peft_model.generate(input_ids=sentence_encoded.input_ids,\n",
    "                            num_beams=1,\n",
    "                            do_sample=True,\n",
    "                            max_new_tokens=1000,\n",
    "                            generation_config=None)[0]  \n",
    "tokenizer.decode(completion, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01255a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mrs. Todd's wife is waiting for her shopping at weekends. Mrs. Todd's wife is waiting for her shopping.\n"
     ]
    }
   ],
   "source": [
    "# PEFT model completion without sampling\n",
    "completion = utils.get_model_completion(prompt, model=peft_model, tokenizer=tokenizer)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b20a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person1#: I'm going to visit you later, Mrs. Todd.\n"
     ]
    }
   ],
   "source": [
    "# Get base model completion\n",
    "completion = utils.get_model_completion(prompt, model=model_orig, tokenizer=tokenizer)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef0a8d",
   "metadata": {},
   "source": [
    "- chk-1246 seems to be better than chk-2000, despite less training\n",
    "- both seem to be better than base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77e491",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_explore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
