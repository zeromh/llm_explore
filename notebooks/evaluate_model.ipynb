{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2026783f",
   "metadata": {},
   "source": [
    "# Evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee2389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import importlib\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = None\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import evaluate\n",
    "\n",
    "from llm_explore import utils\n",
    "from llm_explore.definitions import ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "792bb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User sets dataset and model names\n",
    "DATASET_NAME = \"knkarthick/dialogsum\"\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "PEFT_MODEL_FILE = Path(ROOT_DIR, \"models\", \"peft-dialogue-summary-training-2025-05-08_21-14-28/checkpoint-2000/\")\n",
    "ADAPTER_NAME = \"chk-2000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f245b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned MPS device\n"
     ]
    }
   ],
   "source": [
    "device = utils.get_torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a437cefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and Model Initialization\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16).to(device)\n",
    " # Keep a copy of the original model for later use (keep on CPU for now)\n",
    "model_orig = deepcopy(model).to(torch.device(\"cpu\"))\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model,\n",
    "    PEFT_MODEL_FILE,\n",
    "    torch_device=device,\n",
    "    adapter_name=ADAPTER_NAME,\n",
    "    is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80ae2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load another adapter\n",
    "path = Path(ROOT_DIR, \"models\", \"peft-dialogue-summary-training-2025-05-05_08-03-52/checkpoint-1246/\")\n",
    "peft_model.load_adapter(path, adapter_name=\"chk-1246\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "00a3ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenization and Dataset Preparation\n",
    "def tokenize_function(example):\n",
    "    \"\"\"Tokenizes the input and output text for the model, \n",
    "    including a hardcoded prompt to summarize the conversation.\"\"\"\n",
    "    \n",
    "    start_prompt = \"Summarize the following conversation.\\n\\n\"\n",
    "    end_prompt = \"\\n\\nSummary: \"\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example['dialogue']]\n",
    "    output = tokenizer(prompt, truncation=True, padding='max_length', return_tensors='pt')\n",
    "    output['labels'] = tokenizer(example['summary'], truncation=True, padding='max_length', return_tensors='pt').input_ids\n",
    "    return output\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['id', 'topic', 'dialogue', 'summary'])\n",
    "tokenized_dataset.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e0caa",
   "metadata": {},
   "source": [
    "## Qualitative Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b06d983b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: What's the matter, Bill? You look kind of pale.\n",
      "#Person2#: Oh, I'm just tired.\n",
      "#Person1#: Why?\n",
      "#Person2#: Well, I've been working until around ten every night this week.\n",
      "#Person1#: You should go home at quitting time today and take it easy.\n",
      "#Person2#: Yes. I think I will.\n",
      "#Person1#: That's good. Say, how's your brother?\n",
      "#Person2#: He's fine, but he is awfully busy. He went to the States on a business trip two weeks ago.\n",
      "#Person1#: Oh, really? Is he back yet?\n",
      "#Person2#: No, he won't come back for several more weeks.\n",
      "#Person1#: Wow! He must have a lot to do there.\n",
      "#Person2#: Yes, he does.\n",
      "#Person1#: I want to be sure of the time because I'm going to meet a friend at five o'clock sharp.\n",
      "#Person2#: Well, my watch says 4:30, and that time should be right. I set it with the radio yesterday.\n",
      "#Person1#: Good.\n",
      "\n",
      "Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_id = 105\n",
    "prompt = utils.make_n_shot_summary_prompt(summarize_id=my_id, data=dataset)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9647cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.manual_seed(42)\n",
    "sentence_encoded = tokenizer(prompt, return_tensors='pt').to('mps')  # Move the entire batch to MPS\n",
    "model_orig = model_orig.to(device) \n",
    "peft_model.set_adapter(\"chk-2000\")\n",
    "completion = peft_model.generate(input_ids=sentence_encoded.input_ids,\n",
    "                            num_beams=1,\n",
    "                            do_sample=True,\n",
    "                            max_new_tokens=1000,\n",
    "                            generation_config=None)[0]  # No need to call .to('mps') again\n",
    "tokenizer.decode(completion, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8097d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bill asks Bill if he seems pale and #Person2#'s brother's busy. Bill recommends Bill to go home at quitting time today, and makes sure that his brother comes back from a business trip 2 weeks later.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.set_adapter(\"chk-1246\")\n",
    "completion = peft_model.generate(input_ids=sentence_encoded.input_ids,\n",
    "                            num_beams=1,\n",
    "                            do_sample=True,\n",
    "                            max_new_tokens=1000,\n",
    "                            generation_config=None)[0]  \n",
    "tokenizer.decode(completion, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01255a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bill is tired and wants to go home at quitting time today. Bill's brother is busy and will not come back for several more weeks.\n"
     ]
    }
   ],
   "source": [
    "# PEFT model completion without sampling\n",
    "completion = utils.get_model_completion(prompt, model=peft_model, tokenizer=tokenizer)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "242b20a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: Oh, I'm tired. #Person2#: Oh, I'm tired. #Person1#: I'm tired. #Person2#: I'm tired. #Person1#: I'm tired. #Person2#: I'm tired. #Person1#: I'm tired. #Person2#: I'm tired. #Person1#: I'm tired. #Person2#: I'm tired. #Person1#: I'm tired. #Person2#: I'm tired. #Person1#: I'm tired.\n"
     ]
    }
   ],
   "source": [
    "# Get base model completion\n",
    "completion = utils.get_model_completion(prompt, model=model_orig.to(device), tokenizer=tokenizer)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7216191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bill is tired. Bill and #Person1# talk about Bill's brother.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][my_id]['summary']  # Original summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef0a8d",
   "metadata": {},
   "source": [
    "- chk-1246 seems to be better than chk-2000, despite less training\n",
    "- both seem to be better than base model\n",
    "\n",
    "## Use ROUGE score to compare on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bd9a9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_model_completion(batch, model, tokenizer, device=device):\n",
    "    \"\"\"Get model completion for a batch of inputs.\"\"\"\n",
    "    completion_list = []\n",
    "    for input in batch:\n",
    "        with torch.no_grad():\n",
    "            completion = model.generate(input_ids=input.unsqueeze(0).to(device),\n",
    "                                    num_beams=1,\n",
    "                                    do_sample=False,\n",
    "                                    max_new_tokens=1000,\n",
    "                                    generation_config=None) \n",
    "            completion_list.append(completion)\n",
    "    \n",
    "    decoded_completion_list = []\n",
    "    for comp in completion_list:\n",
    "        decoded_completion = tokenizer.decode(comp[0], skip_special_tokens=True)\n",
    "        decoded_completion_list.append(decoded_completion)\n",
    "    \n",
    "    return decoded_completion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3cbbe322",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.set_adapter(\"chk-2000\")\n",
    "n_samples = 15\n",
    "chk_2000_completions = get_batch_model_completion(tokenized_dataset['test']['input_ids'][:n_samples],\n",
    "    model=peft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device)\n",
    "# 11 minutes to do 124 samples\n",
    "# Maybe let's just do 15 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dcb38847",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.set_adapter(\"chk-1246\")\n",
    "chk_1246_completions = get_batch_model_completion(tokenized_dataset['test']['input_ids'][:n_samples],\n",
    "    model=peft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de85684",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_completions = get_batch_model_completion(tokenized_dataset['test']['input_ids'][:n_samples],\n",
    "    model=model_orig,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "76b6af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['human_summary', 'chk_2000_summary', 'chk_1246_summary', 'base_model_summary']\n",
    "summary_df = pd.DataFrame(dict(human_summary = dataset['test']['summary'][:n_samples],\n",
    "                           chk_2000_summary = chk_2000_completions,\n",
    "                           chk_1246_summary = chk_1246_completions,\n",
    "                           base_model_summary = base_model_completions))\n",
    "summary_df.index.name = 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "77ed07db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_summary</th>\n",
       "      <th>chk_2000_summary</th>\n",
       "      <th>chk_1246_summary</th>\n",
       "      <th>base_model_summary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.</td>\n",
       "      <td>Ms. Dawson asks Ms. Dawson to take a dictation for her. Dawson tells #Person1# that all office communications are restricted to email correspondence and official memos. Dawson tells #Person2# that employees use Instant Messaging to communicate with clients. Dawson tells #Person1# that the memo should be distributed to all employees before 4 pm.</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation for #Person1#.</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.</td>\n",
       "      <td>Ms. Dawson asks Ms. Dawson to take a dictation for her. Dawson tells #Person1# that all office communications are restricted to email correspondence and official memos. Dawson tells #Person2# that employees use Instant Messaging to communicate with clients. Dawson tells #Person1# that the memo should be distributed to all employees before 4 pm.</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation for #Person1#.</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                      human_summary  \\\n",
       "id                                                                                                                                                                                                                    \n",
       "0                                               Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.   \n",
       "1   In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                              chk_2000_summary  \\\n",
       "id                                                                                                                                                                                                                                                                                                                                                               \n",
       "0   Ms. Dawson asks Ms. Dawson to take a dictation for her. Dawson tells #Person1# that all office communications are restricted to email correspondence and official memos. Dawson tells #Person2# that employees use Instant Messaging to communicate with clients. Dawson tells #Person1# that the memo should be distributed to all employees before 4 pm.   \n",
       "1   Ms. Dawson asks Ms. Dawson to take a dictation for her. Dawson tells #Person1# that all office communications are restricted to email correspondence and official memos. Dawson tells #Person2# that employees use Instant Messaging to communicate with clients. Dawson tells #Person1# that the memo should be distributed to all employees before 4 pm.   \n",
       "\n",
       "                                                chk_1246_summary  \\\n",
       "id                                                                 \n",
       "0   #Person1# asks Ms. Dawson to take a dictation for #Person1#.   \n",
       "1   #Person1# asks Ms. Dawson to take a dictation for #Person1#.   \n",
       "\n",
       "                                base_model_summary  \n",
       "id                                                  \n",
       "0   #Person1#: I need to take a dictation for you.  \n",
       "1   #Person1#: I need to take a dictation for you.  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad74108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056795d2516c4c1fb039ec9cba3e92b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fb89a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_rouge = rouge.compute(predictions=summary_df.base_model_summary.tolist(),\n",
    "    references=summary_df.human_summary.tolist(),\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True)\n",
    "chk_2000_rouge = rouge.compute(predictions=summary_df.chk_2000_summary.tolist(),\n",
    "    references=summary_df.human_summary.tolist(),\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True)\n",
    "chk_1246_rouge = rouge.compute(predictions=summary_df.chk_1246_summary.tolist(),\n",
    "    references=summary_df.human_summary.tolist(),\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True)\n",
    "rouge_results = pd.DataFrame(dict(base_model_rouge = base_model_rouge,\n",
    "    chk_2000_rouge = chk_2000_rouge,\n",
    "    chk_1246_rouge = chk_1246_rouge))\n",
    "rouge_results = rouge_results.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "98f0b85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base_model_rouge</th>\n",
       "      <td>0.243815</td>\n",
       "      <td>0.094862</td>\n",
       "      <td>0.224181</td>\n",
       "      <td>0.224980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chk_2000_rouge</th>\n",
       "      <td>0.365717</td>\n",
       "      <td>0.131159</td>\n",
       "      <td>0.281413</td>\n",
       "      <td>0.281743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chk_1246_rouge</th>\n",
       "      <td>0.390178</td>\n",
       "      <td>0.157443</td>\n",
       "      <td>0.324932</td>\n",
       "      <td>0.325072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    rouge1    rouge2    rougeL  rougeLsum\n",
       "base_model_rouge  0.243815  0.094862  0.224181   0.224980\n",
       "chk_2000_rouge    0.365717  0.131159  0.281413   0.281743\n",
       "chk_1246_rouge    0.390178  0.157443  0.324932   0.325072"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "963f8aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the completions to a file\n",
    "summary_df.to_csv(Path(ROOT_DIR, \"data\", \"dialogue_summaries.csv\"), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789ef61c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_explore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
